---
# =============================================================================
# K3s HA Cluster Complete Setup Playbook
# =============================================================================
# This playbook sets up a 3-node K3s HA cluster with:
# - All nodes as control plane + workers
# - KubeVIP for API server HA
# - MetalLB for service load balancing
# - Longhorn for distributed storage
# - ingress-nginx for ingress controller
# - cert-manager for TLS certificates
# - Rancher for cluster management UI
#
# Usage: ansible-playbook site.yml
# =============================================================================

# Phase 1: Prepare all nodes
- name: "Phase 1: Prepare all nodes for K3s"
  hosts: k3s_cluster
  become: true
  gather_facts: true
  any_errors_fatal: true
  roles:
    - prepare
  tags:
    - prepare
    - phase1

# Phase 2: Initialize first master node with K3s
- name: "Phase 2: Initialize K3s on first master"
  hosts: k3s_initial_master
  become: true
  gather_facts: true
  any_errors_fatal: true
  roles:
    - k3s-init
  tags:
    - k3s-init
    - phase2

# Phase 3: Install KubeVIP for API server HA
- name: "Phase 3: Install KubeVIP"
  hosts: k3s_initial_master
  become: true
  gather_facts: true
  any_errors_fatal: true
  roles:
    - kubevip
  tags:
    - kubevip
    - phase3

# Phase 4: Join additional master nodes
- name: "Phase 4: Join additional master nodes"
  hosts: k3s_additional_masters
  become: true
  gather_facts: true
  serial: 1
  any_errors_fatal: true
  roles:
    - k3s-join
  tags:
    - k3s-join
    - phase4

# Phase 4.5: Configure CoreDNS to filter IPv6 (AAAA records)
# This prevents connection delays when IPv6 is disabled at the network level
- name: "Phase 4.5: Configure CoreDNS IPv6 filtering"
  hosts: k3s_initial_master
  become: true
  gather_facts: false
  any_errors_fatal: true
  tags:
    - coredns
    - phase4
  tasks:
    - name: Wait for CoreDNS to be ready
      ansible.builtin.shell: |
        kubectl rollout status deployment/coredns -n kube-system --timeout=120s
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml

    - name: Get current CoreDNS configmap
      ansible.builtin.shell: |
        kubectl get configmap coredns -n kube-system -o jsonpath='{.data.Corefile}'
      register: coredns_config
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml

    - name: Check if IPv6 filtering already configured
      ansible.builtin.set_fact:
        ipv6_filter_exists: "{{ 'template IN AAAA' in coredns_config.stdout }}"

    - name: Patch CoreDNS to filter AAAA records
      ansible.builtin.shell: |
        # Get current Corefile
        COREFILE=$(kubectl get configmap coredns -n kube-system -o jsonpath='{.data.Corefile}')
        
        # Insert template block after 'ready' line
        PATCHED=$(echo "$COREFILE" | awk '/^[[:space:]]*ready[[:space:]]*$/ { print; print "    template IN AAAA {\n      rcode NOERROR\n    }"; next } { print }')
        
        # Apply the patched configmap
        kubectl create configmap coredns -n kube-system \
          --from-literal=Corefile="$PATCHED" \
          --from-literal=NodeHosts="$(kubectl get configmap coredns -n kube-system -o jsonpath='{.data.NodeHosts}')" \
          --dry-run=client -o yaml | kubectl apply -f -
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml
      when: not ipv6_filter_exists

    - name: Restart CoreDNS to apply changes
      ansible.builtin.shell: |
        kubectl rollout restart deployment/coredns -n kube-system
        kubectl rollout status deployment/coredns -n kube-system --timeout=120s
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml
      when: not ipv6_filter_exists

    - name: Display CoreDNS configuration status
      ansible.builtin.debug:
        msg: "{{ 'CoreDNS IPv6 filtering already configured' if ipv6_filter_exists else 'CoreDNS patched to filter IPv6 (AAAA) records' }}"

# Phase 5: Install MetalLB
- name: "Phase 5: Install MetalLB"
  hosts: k3s_initial_master
  become: true
  gather_facts: true
  any_errors_fatal: true
  roles:
    - metallb
  tags:
    - metallb
    - phase5

# Phase 5.5: Wait for MetalLB to be fully ready
- name: "Phase 5.5: Wait for MetalLB readiness"
  hosts: k3s_initial_master
  become: true
  gather_facts: false
  any_errors_fatal: true
  tags:
    - metallb
    - phase5
  tasks:
    - name: Wait for MetalLB controller deployment
      ansible.builtin.shell: |
        kubectl rollout status deployment/controller -n metallb-system --timeout=120s
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml

    - name: Wait for MetalLB speaker pods on all nodes
      ansible.builtin.shell: |
        kubectl wait --for=condition=ready pod -l app=metallb,component=speaker -n metallb-system --timeout=120s
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml

    - name: Pause for MetalLB to stabilize
      ansible.builtin.pause:
        seconds: 15
        prompt: "Waiting for MetalLB to stabilize..."

    - name: Verify MetalLB speakers are running on all nodes
      ansible.builtin.shell: |
        EXPECTED=$(kubectl get nodes --no-headers | wc -l)
        ACTUAL=$(kubectl get pods -n metallb-system -l component=speaker --field-selector=status.phase=Running --no-headers | wc -l)
        if [ "$ACTUAL" -lt "$EXPECTED" ]; then
          echo "Only $ACTUAL of $EXPECTED speaker pods running"
          exit 1
        fi
        echo "All $ACTUAL speaker pods running"
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml

# Phase 6: Prepare Longhorn storage on all nodes
- name: "Phase 6: Prepare Longhorn storage disks"
  hosts: k3s_cluster
  become: true
  gather_facts: true
  any_errors_fatal: true
  roles:
    - longhorn-prep
  tags:
    - longhorn-prep
    - phase6

# Phase 7: Install Longhorn
- name: "Phase 7: Install Longhorn"
  hosts: k3s_initial_master
  become: true
  gather_facts: true
  any_errors_fatal: true
  roles:
    - longhorn
  tags:
    - longhorn
    - phase7

# Phase 8: Install ingress-nginx
- name: "Phase 8: Install ingress-nginx"
  hosts: k3s_initial_master
  become: true
  gather_facts: true
  any_errors_fatal: true
  roles:
    - ingress-nginx
  tags:
    - ingress-nginx
    - phase8

# Phase 9: Install cert-manager
- name: "Phase 9: Install cert-manager"
  hosts: k3s_initial_master
  become: true
  gather_facts: true
  any_errors_fatal: true
  roles:
    - cert-manager
  tags:
    - cert-manager
    - phase9

# Phase 9.5: Wait for cert-manager to be fully ready
- name: "Phase 9.5: Wait for cert-manager readiness"
  hosts: k3s_initial_master
  become: true
  gather_facts: false
  any_errors_fatal: true
  tags:
    - cert-manager
    - rancher
    - phase9
  tasks:
    - name: Wait for cert-manager deployments
      ansible.builtin.shell: |
        kubectl rollout status deployment/cert-manager -n cert-manager --timeout=120s
        kubectl rollout status deployment/cert-manager-webhook -n cert-manager --timeout=120s
        kubectl rollout status deployment/cert-manager-cainjector -n cert-manager --timeout=120s
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml

    - name: Wait for cert-manager webhook to be ready
      ansible.builtin.shell: |
        kubectl wait --for=condition=ready pod -l app.kubernetes.io/component=webhook -n cert-manager --timeout=120s
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml

    - name: Pause for cert-manager webhook to stabilize
      ansible.builtin.pause:
        seconds: 30
        prompt: "Waiting for cert-manager webhook to stabilize before Rancher install..."

    - name: Verify cert-manager is working (create test issuer)
      ansible.builtin.shell: |
        cat <<EOF | kubectl apply -f -
        apiVersion: v1
        kind: Namespace
        metadata:
          name: cert-manager-test
        ---
        apiVersion: cert-manager.io/v1
        kind: Issuer
        metadata:
          name: test-selfsigned
          namespace: cert-manager-test
        spec:
          selfSigned: {}
        EOF
        sleep 5
        kubectl get issuer -n cert-manager-test test-selfsigned -o jsonpath='{.status.conditions[0].status}' | grep -q "True"
        kubectl delete namespace cert-manager-test
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml
      register: cert_manager_test
      retries: 3
      delay: 10
      until: cert_manager_test.rc == 0

    - name: cert-manager verified ready
      ansible.builtin.debug:
        msg: "cert-manager webhook is responding and ready for Rancher installation"

# Phase 10: Install Rancher
- name: "Phase 10: Install Rancher"
  hosts: k3s_initial_master
  become: true
  gather_facts: true
  any_errors_fatal: true
  roles:
    - rancher
  tags:
    - rancher
    - phase10

# Final: Display cluster summary
- name: "Final: Display cluster summary"
  hosts: k3s_initial_master
  become: true
  gather_facts: false
  tasks:
    - name: Get all nodes
      ansible.builtin.shell: kubectl get nodes -o wide
      register: final_nodes
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml

    - name: Get all pods
      ansible.builtin.shell: kubectl get pods -A
      register: final_pods
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml

    - name: Get services
      ansible.builtin.shell: kubectl get svc -A
      register: final_services
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml

    - name: Get Longhorn UI IP
      ansible.builtin.shell: |
        kubectl get svc longhorn-frontend-lb -n longhorn-system -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "pending"
      register: longhorn_ui_ip
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml

    - name: Get ingress-nginx IP
      ansible.builtin.shell: |
        kubectl get svc ingress-nginx-controller -n ingress-nginx -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "pending"
      register: ingress_ip
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml

    - name: Display final cluster summary
      ansible.builtin.debug:
        msg: |
          
          ╔══════════════════════════════════════════════════════════════════════╗
          ║                    K3S HA CLUSTER SETUP COMPLETE!                   ║
          ╠══════════════════════════════════════════════════════════════════════╣
          ║  Cluster Nodes:                                                      ║
          ╚══════════════════════════════════════════════════════════════════════╝
          {{ final_nodes.stdout }}
          
          ╔══════════════════════════════════════════════════════════════════════╗
          ║  Key Information:                                                    ║
          ╠══════════════════════════════════════════════════════════════════════╣
          ║  API Server VIP (KubeVIP): https://{{ kubevip_vip }}:6443
          ║  MetalLB IP Range: {{ metallb_ip_range }}
          ║  
          ║  Web UIs:
          ║    Longhorn UI: http://{{ longhorn_ui_ip.stdout }}
          ║    Rancher UI:  https://{{ rancher_hostname }}
          ║  
          ║  Rancher Bootstrap Password: {{ rancher_bootstrap_password }}
          ║  (You will be prompted to change this on first login)
          ║  
          ║  IP Assignments:
          ║    KubeVIP (API):      {{ kubevip_vip }}
          ║    Longhorn UI:        {{ longhorn_ui_ip.stdout }}
          ║    ingress-nginx:      {{ ingress_ip.stdout }}
          ║  
          ║  Kubeconfig Location: ~/.kube/config (on each node)
          ║  
          ║  To access cluster from your workstation:
          ║    scp ansible@192.168.50.41:~/.kube/config ~/.kube/config
          ║    
          ║  DNS Required:
          ║    {{ rancher_hostname }} -> {{ ingress_ip.stdout }}
          ╚══════════════════════════════════════════════════════════════════════╝
  tags:
    - summary
    - always
